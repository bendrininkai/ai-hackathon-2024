{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is to select a single set of district, age group and gender, run through many hyperparameters, cluster them and select optimal scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import logging\n",
    "import warnings\n",
    "import contextlib\n",
    "from itertools import product\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting rid of warning and logging messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress logging messages from cmdstanpy\n",
    "logger = logging.getLogger('cmdstanpy')\n",
    "logger.setLevel(logging.ERROR)\n",
    "for handler in logger.handlers:\n",
    "    handler.setLevel(logging.ERROR)\n",
    "\n",
    "# Suppress SettingWithCopyWarning\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context manager to suppress logging\n",
    "@contextlib.contextmanager\n",
    "def suppress_logging():\n",
    "    logging.disable(logging.CRITICAL)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        logging.disable(logging.NOTSET)\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess(train_file_path, test_file_path):\n",
    "    train_data = pd.read_csv(train_file_path)\n",
    "    test_data = pd.read_csv(test_file_path)\n",
    "\n",
    "    train_data['as_of_date_id'] = train_data['as_of_date_id'].astype(int)\n",
    "    train_data['age_bin_id'] = train_data['age_bin_id'].astype(str)\n",
    "    train_data['gender_id'] = train_data['gender_id'].astype(str)\n",
    "    train_data['district_id'] = train_data['district_id'].astype(str)\n",
    "\n",
    "    test_data['as_of_date_id'] = test_data['as_of_date_id'].astype(int)\n",
    "    test_data['age_bin_id'] = test_data['age_bin_id'].astype(str)\n",
    "    test_data['gender_id'] = test_data['gender_id'].astype(str)\n",
    "    test_data['district_id'] = test_data['district_id'].astype(str)\n",
    "\n",
    "    for age_bin in train_data['age_bin_id'].unique():\n",
    "        for gender in train_data['gender_id'].unique():\n",
    "            for district in train_data['district_id'].unique():\n",
    "                mask = (train_data['age_bin_id'] == age_bin) & (train_data['gender_id'] == gender) & (train_data['district_id'] == district)\n",
    "                count_75 = train_data.loc[mask & (train_data['as_of_date_id'] == 75), 'count'].values\n",
    "                count_77 = train_data.loc[mask & (train_data['as_of_date_id'] == 77), 'count'].values\n",
    "                if len(count_75) > 0 and len(count_77) > 0:\n",
    "                    avg_count = (count_75[0] + count_77[0]) / 2\n",
    "                    train_data.loc[mask & (train_data['as_of_date_id'] == 76), 'count'] = avg_count\n",
    "\n",
    "    # Filter train_data to start from as_of_date_id 70\n",
    "    train_data = train_data[train_data['as_of_date_id'] >= 70].reset_index(drop=True)\n",
    "\n",
    "    # Assume start date and convert 'as_of_date_id' to datetime\n",
    "    start_date = pd.to_datetime('2000-01-01')\n",
    "    train_data['ds'] = start_date + pd.to_timedelta(train_data['as_of_date_id'], unit='D')\n",
    "    test_data['ds'] = start_date + pd.to_timedelta(test_data['as_of_date_id'], unit='D')\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "# Function to normalize data\n",
    "def normalize_data(df, column):\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    df[column] = (df[column] - mean) / std\n",
    "    return mean, std\n",
    "\n",
    "# Function to denormalize data\n",
    "def denormalize_data(df, column, mean, std):\n",
    "    df[column] = df[column] * std + mean\n",
    "    return df\n",
    "\n",
    "# Function to train a model for a specific scenario and hyperparameters\n",
    "def train_model(train_data, district, age_bin, gender, hyperparams):\n",
    "    mask = (train_data['district_id'] == district) & (train_data['age_bin_id'] == age_bin) & (train_data['gender_id'] == gender)\n",
    "    subset_data = train_data[mask]\n",
    "\n",
    "    # Check if subset_data has at least 2 non-NaN rows\n",
    "    if len(subset_data) < 2:\n",
    "        return None, None, None\n",
    "\n",
    "    # Normalize the data\n",
    "    mean, std = normalize_data(subset_data, 'count')\n",
    "    \n",
    "    subset_data = subset_data.rename(columns={'ds': 'ds', 'count': 'y'})\n",
    "    model = Prophet(\n",
    "        yearly_seasonality=hyperparams['yearly_seasonality'],\n",
    "        changepoint_prior_scale=hyperparams['changepoint_prior_scale'],\n",
    "        seasonality_prior_scale=hyperparams['seasonality_prior_scale'],\n",
    "        changepoint_range=hyperparams['changepoint_range']\n",
    "    )\n",
    "    model.add_seasonality(\n",
    "        name='12-period',\n",
    "        period=12, \n",
    "        fourier_order=hyperparams['fourier_order']\n",
    "    )\n",
    "\n",
    "    with suppress_logging():\n",
    "        model.fit(subset_data[['ds', 'y']])\n",
    "    \n",
    "    return model, mean, std\n",
    "\n",
    "# Function to calculate the RMSE for the training dataset\n",
    "def calculate_rmse(model, train_data, district, age_bin, gender):\n",
    "    mask_train = (train_data['district_id'] == district) & (train_data['age_bin_id'] == age_bin) & (train_data['gender_id'] == gender)\n",
    "    subset_train_data = train_data[mask_train]\n",
    "\n",
    "    train_forecast = model.predict(subset_train_data[['ds']])\n",
    "    rmse = np.sqrt(np.mean((train_forecast['yhat'].values - subset_train_data['count'].values)**2))\n",
    "    return rmse\n",
    "\n",
    "# Function to train and evaluate a model for a specific scenario and hyperparameters\n",
    "def train_and_evaluate_model(train_data, test_data, district, age_bin, gender, hyperparam_tuple):\n",
    "    hyperparams = dict(zip(hyperparameter_space.keys(), hyperparam_tuple))\n",
    "    model, mean, std = train_model(train_data, district, age_bin, gender, hyperparams)\n",
    "    if model is not None:\n",
    "        rmse = calculate_rmse(model, train_data, district, age_bin, gender)\n",
    "        return model, mean, std, rmse, hyperparams\n",
    "    return None\n",
    "\n",
    "# Function to make predictions for a specific scenario\n",
    "def make_scenario_predictions(model, mean, std, test_data, district, age_bin, gender):\n",
    "    mask_test = (test_data['district_id'] == district) & (test_data['age_bin_id'] == age_bin) & (test_data['gender_id'] == gender)\n",
    "    subset_test_data = test_data[mask_test]\n",
    "\n",
    "    future = subset_test_data[['ds']]\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    # Denormalize the predictions\n",
    "    forecast['yhat'] = forecast['yhat'] * std + mean\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "        subset_test_data['Prediction'] = forecast['yhat'].values\n",
    "        subset_test_data['Prediction'] = subset_test_data['Prediction'].iloc[::-1].values\n",
    "\n",
    "    return subset_test_data[['ID', 'district_id', 'age_bin_id', 'gender_id', 'as_of_date_id', 'Prediction']]\n",
    "\n",
    "def cluster_predictions(models, test_data, district, age_bin, gender, n_clusters=5):\n",
    "    predictions_list = []\n",
    "    model_info = []\n",
    "\n",
    "    for model, mean, std, rmse, hyperparams in models:\n",
    "        predictions = make_scenario_predictions(model, mean, std, test_data, district, age_bin, gender)\n",
    "        preds = predictions['Prediction'].values\n",
    "        predictions_list.append(preds)\n",
    "        model_info.append((model, mean, std, rmse, hyperparams))\n",
    "\n",
    "    predictions_array = np.array(predictions_list)\n",
    "\n",
    "    # Perform clustering\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n",
    "    labels = clustering.fit_predict(predictions_array)\n",
    "\n",
    "    # Select one model per cluster\n",
    "    unique_models = []\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_indices = np.where(labels == cluster_id)[0]\n",
    "        selected_index = cluster_indices[0]  # Select the first model in each cluster\n",
    "        unique_models.append(model_info[selected_index])\n",
    "\n",
    "    return unique_models\n",
    "\n",
    "\n",
    "def print_top_models_in_cluster(models, test_data, district, age_bin, gender, n_clusters=5):\n",
    "    predictions_list = []\n",
    "    model_info = []\n",
    "\n",
    "    for model, mean, std, rmse, hyperparams in models:\n",
    "        predictions = make_scenario_predictions(model, mean, std, test_data, district, age_bin, gender)\n",
    "        preds = predictions['Prediction'].values\n",
    "        predictions_list.append(preds)\n",
    "        model_info.append((model, mean, std, rmse, hyperparams))\n",
    "\n",
    "    predictions_array = np.array(predictions_list)\n",
    "\n",
    "    # Perform clustering\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n",
    "    labels = clustering.fit_predict(predictions_array)\n",
    "\n",
    "    # Dictionary to store models in each cluster\n",
    "    cluster_dict = {i: [] for i in range(n_clusters)}\n",
    "    for i, label in enumerate(labels):\n",
    "        cluster_dict[label].append(model_info[i])\n",
    "\n",
    "    return cluster_dict\n",
    "\n",
    "def display_top_models(cluster_dict, cluster_id, top_n=5):\n",
    "    if cluster_id in cluster_dict:\n",
    "        cluster_models = cluster_dict[cluster_id]\n",
    "        # Sort models in the cluster based on RMSE\n",
    "        cluster_models.sort(key=lambda x: x[3])\n",
    "        top_models = cluster_models[:top_n]\n",
    "\n",
    "        print(f\"Top {top_n} models in cluster {cluster_id}:\")\n",
    "        for i, (model, mean, std, rmse, hyperparams) in enumerate(top_models):\n",
    "            print(f\"Model {i+1}: RMSE = {rmse}, Hyperparameters = {hyperparams}\")\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id} not found.\")\n",
    "\n",
    "def plot_predictions_for_selected_model(models, test_data, district, age_bin, gender, selected_hyperparams):\n",
    "    # Find the model with the specified hyperparameters\n",
    "    selected_model_info = None\n",
    "    for model, mean, std, rmse, hyperparams in models:\n",
    "        if hyperparams == selected_hyperparams:\n",
    "            selected_model_info = (model, mean, std, rmse, hyperparams)\n",
    "            break\n",
    "    \n",
    "    if selected_model_info is None:\n",
    "        print(\"Model with the specified hyperparameters not found.\")\n",
    "        return\n",
    "    \n",
    "    model, mean, std, rmse, hyperparams = selected_model_info\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = make_scenario_predictions(model, mean, std, test_data, district, age_bin, gender)\n",
    "    \n",
    "    # Add training data\n",
    "    train_subset = train_data[(train_data['district_id'] == district) & \n",
    "                              (train_data['age_bin_id'] == age_bin) & \n",
    "                              (train_data['gender_id'] == gender)]\n",
    "    \n",
    "    # Fit a linear regression to the training data\n",
    "    X_train = train_subset['as_of_date_id'].values.reshape(-1, 1)\n",
    "    y_train = train_subset['count'].values\n",
    "    linear_regressor = LinearRegression()\n",
    "    linear_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Create a range for the trend line from the start of training to the end of predictions\n",
    "    start_train = train_subset['as_of_date_id'].min()\n",
    "    end_pred = predictions['as_of_date_id'].max()\n",
    "    X_trend = np.arange(start_train, end_pred + 1).reshape(-1, 1)\n",
    "    y_trend = linear_regressor.predict(X_trend)\n",
    "    \n",
    "    # Create a dataframe for the trend line\n",
    "    trend_df = pd.DataFrame({\n",
    "        'as_of_date_id': X_trend.flatten(),\n",
    "        'trend': y_trend\n",
    "    })\n",
    "\n",
    "    # Plot the predictions\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add training data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=train_subset['as_of_date_id'], \n",
    "        y=train_subset['count'], \n",
    "        mode='lines+markers', \n",
    "        name='Training Data',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "\n",
    "    # Add prediction data\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=predictions['as_of_date_id'], \n",
    "        y=predictions['Prediction'], \n",
    "        mode='lines+markers', \n",
    "        name='Predictions',\n",
    "        line=dict(color='red')\n",
    "    ))\n",
    "\n",
    "    # Add trend line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=trend_df['as_of_date_id'], \n",
    "        y=trend_df['trend'], \n",
    "        mode='lines', \n",
    "        name='Trend Line',\n",
    "        line=dict(color='rgba(0, 0, 0, 0.5)', width=1, dash=\"dash\"),\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "    # Calculate the min and max values for y-axis\n",
    "    all_y_values = pd.concat([train_subset['count'], predictions['Prediction'], trend_df['trend']])\n",
    "    y_min = all_y_values.min() * 0.99\n",
    "    y_max = all_y_values.max() * 1.01\n",
    "\n",
    "    # Add vertical lines for every 12 points\n",
    "    vertical_lines = range(start_train + 12, end_pred, 12)\n",
    "    for period in vertical_lines:\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=period,\n",
    "            y0=y_min,\n",
    "            x1=period,\n",
    "            y1=y_max,\n",
    "            line=dict(color=\"Green\", width=2, dash=\"dash\")\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Population Count per Period of Time (District: {district}, Age Bin: {age_bin}, Gender: {gender})',\n",
    "        xaxis_title='Time Period',\n",
    "        yaxis_title='Population Count',\n",
    "        legend_title='Dataset',\n",
    "        height=500\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "def display_top_hyperparams_with_extreme_sums(cluster_dict, cluster_id, top_n=3):\n",
    "    if cluster_id in cluster_dict:\n",
    "        cluster_models = cluster_dict[cluster_id]\n",
    "        hyperparams_with_sums = []\n",
    "\n",
    "        for model, mean, std, rmse, hyperparams in cluster_models:\n",
    "            predictions = make_scenario_predictions(model, mean, std, test_data, district, age_bin, gender)\n",
    "            total_sum = predictions['Prediction'].sum()\n",
    "            hyperparams_with_sums.append((hyperparams, total_sum))\n",
    "\n",
    "        # Sort by the sum of predicted values\n",
    "        hyperparams_with_sums.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"Top {top_n} hyperparameter sets with highest sums in cluster {cluster_id}:\")\n",
    "        for i in range(min(top_n, len(hyperparams_with_sums))):\n",
    "            print(f\"Model {i+1}: Hyperparameters = {hyperparams_with_sums[i][0]}, Sum of Predictions = {hyperparams_with_sums[i][1]}\")\n",
    "\n",
    "        hyperparams_with_sums.sort(key=lambda x: x[1])\n",
    "        \n",
    "        print(f\"\\nTop {top_n} hyperparameter sets with lowest sums in cluster {cluster_id}:\")\n",
    "        for i in range(min(top_n, len(hyperparams_with_sums))):\n",
    "            print(f\"Model {i+1}: Hyperparameters = {hyperparams_with_sums[i][0]}, Sum of Predictions = {hyperparams_with_sums[i][1]}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Cluster {cluster_id} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main file to run a specific scenario through many possible hyperparameters and then cluster them to investigate specified cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == '__main__':\n",
    "    train_file_path = 'data/train.csv'\n",
    "    test_file_path = 'data/test.csv'\n",
    "\n",
    "    # Load and preprocess data\n",
    "    train_data, test_data = load_and_preprocess(train_file_path, test_file_path)\n",
    "\n",
    "    # Enter the desired scenario (district, age bin, and gender)\n",
    "    district = '19'\n",
    "    age_bin = '3'\n",
    "    gender = '0'\n",
    "\n",
    "    # Define the hyperparameter search space\n",
    "    hyperparameter_space = {\n",
    "        'yearly_seasonality': ['auto'],\n",
    "        'changepoint_prior_scale': [0.001, 0.01, 0.03, 0.05, 0.075, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0],\n",
    "        'seasonality_prior_scale': [1, 5, 10, 15, 20],\n",
    "        'changepoint_range': [0.5, 0.6, 0.7, 0.8, 0.9, 0.95],\n",
    "        'fourier_order': [1, 3, 5, 7, 10, 15, 20]\n",
    "    }\n",
    "\n",
    "    # Generate all possible combinations of hyperparameters\n",
    "    hyperparameter_combinations = list(product(*hyperparameter_space.values()))\n",
    "\n",
    "    # Get the total number of CPU cores\n",
    "    total_cores = os.cpu_count()\n",
    "    \n",
    "    # Calculate the number of cores to use, leaving 5 cores spare\n",
    "    num_cores = max(1, total_cores - 5)\n",
    "    \n",
    "    # Create a pool of worker processes with the specified number of cores\n",
    "    pool = multiprocessing.Pool(processes=num_cores)\n",
    "\n",
    "    # Create a partial function with fixed arguments\n",
    "    train_and_evaluate_partial = partial(train_and_evaluate_model, train_data, test_data, district, age_bin, gender)\n",
    "\n",
    "    # Train and evaluate models with different hyperparameter combinations in parallel\n",
    "    results = pool.map(train_and_evaluate_partial, hyperparameter_combinations)\n",
    "\n",
    "    # Close the pool of worker processes\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Filter out None results and create the models list\n",
    "    models = [result for result in results if result is not None]\n",
    "\n",
    "    # Sort the models based on their performance (RMSE)\n",
    "    models.sort(key=lambda x: x[3])\n",
    "\n",
    "    # Cluster predictions to get unique models\n",
    "    unique_models = cluster_predictions(models, test_data, district, age_bin, gender, n_clusters=5)\n",
    "\n",
    "    # Plot the predictions for the unique models\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add training data\n",
    "    train_subset = train_data[(train_data['district_id'] == district) & \n",
    "                            (train_data['age_bin_id'] == age_bin) & \n",
    "                            (train_data['gender_id'] == gender)]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=train_subset['as_of_date_id'], \n",
    "        y=train_subset['count'], \n",
    "        mode='lines+markers', \n",
    "        name='Training Data',\n",
    "        line=dict(color='blue')\n",
    "    ))\n",
    "\n",
    "    # Fit a linear regression to the training data\n",
    "    X_train = train_subset['as_of_date_id'].values.reshape(-1, 1)\n",
    "    y_train = train_subset['count'].values\n",
    "    linear_regressor = LinearRegression()\n",
    "    linear_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Create a range for the trend line from the start of training to the end of predictions\n",
    "    start_train = train_subset['as_of_date_id'].min()\n",
    "    end_pred = test_data['as_of_date_id'].max()\n",
    "    X_trend = np.arange(start_train, end_pred + 1).reshape(-1, 1)\n",
    "    y_trend = linear_regressor.predict(X_trend)\n",
    "    \n",
    "    # Create a dataframe for the trend line\n",
    "    trend_df = pd.DataFrame({\n",
    "        'as_of_date_id': X_trend.flatten(),\n",
    "        'trend': y_trend\n",
    "    })\n",
    "\n",
    "    # Add prediction data for each unique model\n",
    "    colors = ['red', 'green', 'orange', 'purple', 'black']\n",
    "    for i, (model, mean, std, _, _) in enumerate(unique_models):\n",
    "        predictions = make_scenario_predictions(model, mean, std, test_data, district, age_bin, gender)\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=predictions['as_of_date_id'], \n",
    "            y=predictions['Prediction'], \n",
    "            mode='lines+markers', \n",
    "            name=f'Predictions (Model {i+1})',\n",
    "            line=dict(color=colors[i])\n",
    "        ))\n",
    "\n",
    "    # Add trend line\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=trend_df['as_of_date_id'], \n",
    "        y=trend_df['trend'], \n",
    "        mode='lines', \n",
    "        name='Trend Line',\n",
    "        line=dict(color='rgba(0, 0, 0, 0.5)', width=1, dash=\"dash\"),\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "    # Calculate the min and max values for y-axis\n",
    "    all_y_values = pd.concat([train_subset['count']] + [make_scenario_predictions(model, mean, std, test_data, district, age_bin, gender)['Prediction'] for model, mean, std, _, _ in unique_models] + [trend_df['trend']])\n",
    "    y_min = all_y_values.min() * 0.99\n",
    "    y_max = all_y_values.max() * 1.01\n",
    "\n",
    "    # Add vertical lines for every 12 points\n",
    "    vertical_lines = range(start_train + 12, end_pred, 12)\n",
    "    for period in vertical_lines:\n",
    "        fig.add_shape(\n",
    "            type=\"line\",\n",
    "            x0=period,\n",
    "            y0=y_min,\n",
    "            x1=period,\n",
    "            y1=y_max,\n",
    "            line=dict(color=\"Green\", width=2, dash=\"dash\")\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'Population Count per Period of Time (District: {district}, Age Bin: {age_bin}, Gender: {gender})',\n",
    "        xaxis_title='Time Period',\n",
    "        yaxis_title='Population Count',\n",
    "        legend_title='Dataset',\n",
    "        height=500\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select cluster (model) number to get optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster predictions to get unique models\n",
    "cluster_dict = print_top_models_in_cluster(models, test_data, district, age_bin, gender, n_clusters=5)\n",
    "\n",
    "# Display top models in a specific cluster\n",
    "model_nr = 3\n",
    "selected_cluster_id = model_nr - 1  # Change this to the desired cluster ID\n",
    "# display_top_models(cluster_dict, selected_cluster_id, top_n=5)\n",
    "\n",
    "# Display top hyperparameter sets with the highest and lowest sums of predicted values\n",
    "display_top_hyperparams_with_extreme_sums(cluster_dict, selected_cluster_id, top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a set of huperparameters to plot predictions for this specific case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters of the model you want to plot\n",
    "selected_hyperparams = {'yearly_seasonality': 'auto', 'changepoint_prior_scale': 0.7, 'seasonality_prior_scale': 1, 'changepoint_range': 0.95, 'fourier_order': 7}\n",
    "\n",
    "# Call the function to plot predictions for the selected model\n",
    "plot_predictions_for_selected_model(models, test_data, district, age_bin, gender, selected_hyperparams)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
